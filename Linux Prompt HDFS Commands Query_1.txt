#QUERY 1
#Mi posiziono nella cartella in locale dove ho salvato i dataset "somministrazioni-vaccini-summary-latest.csv" e "punti-somministrazione-tipologia.csv" ed il il file "query_1.py"

# Apri Cartella Corrente nel terminale 

#Avvio HDFS 
$HADOOP_HOME/sbin/start-dfs.sh 

#Avvio Standalone Master Server Spark
$SPARK_HOME/sbin/start-master.sh

# Eseguo il comando makerdir su HDFS per creare la cartella e sottocartella dove verr√† caricato il dataset di input
hdfs dfs -mkdir -p /user/query

# Carico i file "somministrazioni-vaccini-summary-latest.csv" e "punti-somministrazione-tipologia.csv" prensenti in locale su HDFS nel path creato allo step precedente

hdfs dfs -put somministrazioni-vaccini-summary-latest.csv /user/query/somministrazioni-vaccini-summary-latest.csv

hdfs dfs -put punti-somministrazione-tipologia.csv /user/query/punti-somministrazione-tipologia.csv

# Verifico che i file siano presenti sulla cartella indicata in HDFS 
hdfs dfs -ls /user/query

# Eseguo lo script "query_1_final.py"
spark-submit query_1.py

# Verifico il file di output generato nella cartella creata su HDFS da parte del codice query_1.py
hdfs dfs -ls /user/query/output_query_1

# Leggo i risultati della query1
hdfs dfs -cat /user/query/output_query_1/part-00000

# Copio il file di output da HDFS  in locale 
hdfs dfs -get /user/query/output_query_1/part-00000 output_query_1.txt

#Per poter rieseguire il codice con dati aggiornati cancello cartella di output e Input
# Cancello cartella e contenuto di output su HDFS
hdfs dfs -rm -r  /user/query/output_query_1
# Cancello cartella e contenuto di Input su HDFS
hdfs dfs -rm -r  /user/query

#Fermo Standalone Master Server Spark
$SPARK_HOME/sbin/stop-master.sh

#Fermo HDFS 
$HADOOP_HOME/sbin/stop-dfs.sh